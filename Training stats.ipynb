{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import Utils\n",
    "import AG_RN\n",
    "import importlib\n",
    "importlib.reload(Utils)\n",
    "from Utils import DataGenerator\n",
    "from Utils import TimingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the accuracy of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una lista con X valores aleatorios entre el 0 y el número de huellas\n",
    "# que tenemos para seleccionar aleatoriamente los utilizados para las pruebas.\n",
    "def testModel(md,th,x_val,label_val,x_real,y_real,dictionary,verb = 0):\n",
    "    model = md\n",
    "    treshold = th\n",
    "    x_r = x_real\n",
    "    y_r = y_real\n",
    "    labelPositionDictionary = dictionary\n",
    "    verbose = verb\n",
    "    \n",
    "    random_idxs = []\n",
    "    for i in range(0,100):\n",
    "        random_idxs.append(random.randint(0, len(x_val)-1))\n",
    "\n",
    "    #print (random_idxs)\n",
    "\n",
    "    result =  []\n",
    "    for random_idx in random_idxs:\n",
    "        #Para cada uno de los índices generados aleatoriamente  obtenemos su imagen y su correspondiente etiqueta\n",
    "        random_img = x_val[random_idx]\n",
    "        random_label = label_val[random_idx]\n",
    "\n",
    "        #Para la imagen que hemos obtenido vamos a aplicar un desenfoque para simular la lectura de la misma.\n",
    "        random_img = Utils.dataAugmentationImg(random_img)\n",
    "\n",
    "        # Vamos a obtener la imagen correspondiente a la etiqueta para obtener la huella con la que comparar.\n",
    "        match_key = str(random_label)\n",
    "\n",
    "        rx = x_r[labelPositionDictionary[match_key]].reshape((1, 90, 90, 1)).astype(np.float32) / 255.\n",
    "        ry = y_r[labelPositionDictionary[match_key]]\n",
    "\n",
    "        # Corremos el modelo sobre esta huella\n",
    "        pred_rx = model.predict([random_img, rx])\n",
    "\n",
    "        # Ahora vamos a obtener una imagen distinta para realizar una comparación que debería de ser errónea\n",
    "        unmatch_key, unmatch_idx = random.choice(list(labelPositionDictionary.items()))\n",
    "\n",
    "        ux = x_r[unmatch_idx].reshape((1, 90, 90, 1)).astype(np.float32) / 255.\n",
    "        uy = y_r[unmatch_idx]\n",
    "\n",
    "        # Corremos el modelo sobre esta huella\n",
    "        pred_ux = model.predict([random_img, ux])\n",
    "\n",
    "        if verbose:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title('Input: %s' %random_label)\n",
    "            plt.imshow(random_img.squeeze(), cmap='gray')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title('O: %.02f, %s' % (pred_rx, ry))\n",
    "            plt.imshow(rx.squeeze(), cmap='gray')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title('X: %.02f, %s' % (pred_ux, uy))\n",
    "            plt.imshow(ux.squeeze(), cmap='gray')\n",
    "\n",
    "        result.append([label_val[random_idx],pred_rx,pred_ux ])\n",
    "        \n",
    "    FRR = 0\n",
    "    FAR = 0\n",
    "    for r in result:\n",
    "        if r[1] < treshold:\n",
    "            #print ('FRR')\n",
    "            FRR = FRR +1\n",
    "        if r[2] > treshold:\n",
    "            #print ('FAR')\n",
    "            FAR = FAR + 1\n",
    "\n",
    "    FRR_porc = FRR / (len(result) )\n",
    "    \n",
    "    FAR_porc = FAR / (len(result) )\n",
    "    if verbose:\n",
    "        print ( 'EL FRR es de: ', FRR_porc * 100,' %')\n",
    "        print ( 'EL FAR es de: ', FAR_porc * 100,' %')\n",
    "\n",
    "    return result, FRR, FAR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store and save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeResults(dataframe, dataset_size, nEpochs, tiempo, FRR, FAR):\n",
    "    #new_df = pd.DataFrame([[1, 2, 3, 4, 5]],columns = ['dataset_size', 'nEpochs', 'tiempo', 'FRR', 'FAR'])\n",
    "    new_df = pd.DataFrame([[dataset_size, nEpochs, tiempo, tiempo/60, FRR, FAR]],columns = ['dataset_size', 'nEpochs', 'tiempo','tiempo(m)', 'FRR', 'FAR'])\n",
    "    return dataframe.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(fn,df):\n",
    "    df.to_excel(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL  (6000, 90, 90) (6000, 4)\n",
      "Easy  (17931, 90, 90) (17931, 4)\n",
      "Medium  (17067, 90, 90) (17067, 4)\n",
      "Hard  (14272, 90, 90) (14272, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el Dataset total\n",
    "\n",
    "ruta  = 'dataset_600/'\n",
    "\n",
    "x_real = np.load(ruta + 'img_real.npz')['data']\n",
    "y_real = np.load(ruta + 'label_real.npy')\n",
    "x_easy = np.load(ruta + 'img_easy.npz')['data']\n",
    "y_easy = np.load(ruta + 'label_easy.npy')\n",
    "x_medium = np.load(ruta + 'img_medium.npz')['data']\n",
    "y_medium = np.load(ruta + 'label_medium.npy')\n",
    "x_hard = np.load(ruta + 'img_hard.npz')['data']\n",
    "y_hard = np.load(ruta + 'label_hard.npy')\n",
    "\n",
    "print('REAL ',x_real.shape, y_real.shape)\n",
    "print('Easy ',x_easy.shape, y_easy.shape)\n",
    "print('Medium ',x_medium.shape, y_medium.shape)\n",
    "print('Hard ',x_hard.shape, y_hard.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generando un set de datos de  20  individuos\n",
      "Model: \"Model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 90, 90, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 90, 90, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "featureModel (Model)            (None, 22, 22, 32)   9568        input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 22, 22, 32)   0           featureModel[1][0]               \n",
      "                                                                 featureModel[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 22, 22, 32)   9248        subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3872)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           247872      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 266,753\n",
      "Trainable params: 266,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "4/4 [==============================] - 1s 171ms/step - loss: 0.6968 - acc: 0.4453\n",
      "Epoch 2/15\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.6752 - acc: 0.6406\n",
      "Epoch 3/15\n",
      "4/4 [==============================] - 1s 201ms/step - loss: 0.6586 - acc: 0.5859\n",
      "Epoch 4/15\n",
      "4/4 [==============================] - 1s 193ms/step - loss: 0.6493 - acc: 0.5078\n",
      "Epoch 5/15\n",
      "4/4 [==============================] - 1s 185ms/step - loss: 0.6207 - acc: 0.6641\n",
      "Epoch 6/15\n",
      "4/4 [==============================] - 1s 186ms/step - loss: 0.5466 - acc: 0.7891\n",
      "Epoch 7/15\n",
      "4/4 [==============================] - 1s 179ms/step - loss: 0.4766 - acc: 0.8281\n",
      "Epoch 8/15\n",
      "4/4 [==============================] - 1s 176ms/step - loss: 0.4521 - acc: 0.7812\n",
      "Epoch 9/15\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.4411 - acc: 0.8125\n",
      "Epoch 10/15\n",
      "4/4 [==============================] - 1s 178ms/step - loss: 0.4724 - acc: 0.7344\n",
      "Epoch 11/15\n",
      "4/4 [==============================] - 1s 177ms/step - loss: 0.4443 - acc: 0.7969\n",
      "Epoch 12/15\n",
      "4/4 [==============================] - 1s 207ms/step - loss: 0.3663 - acc: 0.8672\n",
      "Epoch 13/15\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 0.2816 - acc: 0.8828\n",
      "Epoch 14/15\n",
      "4/4 [==============================] - 1s 194ms/step - loss: 0.3775 - acc: 0.8047\n",
      "Epoch 15/15\n",
      "4/4 [==============================] - 1s 180ms/step - loss: 0.4166 - acc: 0.8203\n",
      "\n",
      "\n",
      "El tiempo total de entrenamiento para esta red sobre un set de datos de  20  individuos ha sido de  16.534206000040285  segundos\n"
     ]
    }
   ],
   "source": [
    "#Principales parámetros\n",
    "dataset_sizes = [20]#,30,40,50,60,80,100,150,200,250,300,400,500,1000,2000,3000,4000,5000,6000]\n",
    "nEpochs = 15\n",
    "hard = 1\n",
    "\n",
    "#Inicializamos variables\n",
    "models = []\n",
    "labelPositionDictionaries = []\n",
    "resultsDF = pd.read_excel('Resultados.xlsx')\n",
    "results = []\n",
    "for dataset_size in dataset_sizes:\n",
    "    #Generamos un dataset con el tamaño indicado\n",
    "    x_r,y_r,x_e,y_e,x_m, y_m, x_h, y_h = Utils.createDataset_X(x_real,y_real,x_easy,y_easy,x_medium, y_medium, x_hard, y_hard, dataset_size)\n",
    "    \n",
    "    print('\\n\\nGenerando un set de datos de ',dataset_size,' individuos')\n",
    "\n",
    "    \n",
    "    #Combinamos las huellas alteradas\n",
    "    if hard:\n",
    "        x_data = np.concatenate([x_e, x_m, x_h], axis=0)\n",
    "        y_data = np.concatenate([y_e, y_m, y_h], axis=0)\n",
    "    else:\n",
    "        x_data = np.concatenate([x_e, x_m], axis=0)\n",
    "        y_data = np.concatenate([y_e, y_m], axis=0)\n",
    "\n",
    "    #Partimos los datos entre Entrenamiento y Test\n",
    "    x_train, x_val, label_train, label_val = train_test_split(x_data, y_data, test_size=0.1)\n",
    "\n",
    "    #Creamos el diccionario\n",
    "    labelPositionDictionary = {}\n",
    "\n",
    "    for i, y in enumerate(y_r):\n",
    "        label = str(y)\n",
    "        labelPositionDictionary[label] = i\n",
    "    labelPositionDictionaries.append(labelPositionDictionary)\n",
    "    \n",
    "    #Generamos los datos de entrenamiento\n",
    "    \n",
    "    train_gen = DataGenerator(x_train, label_train, x_r, labelPositionDictionary, shuffle=True)\n",
    "    val_gen = DataGenerator(x_val, label_val, x_r, labelPositionDictionary, shuffle=False)\n",
    "    \n",
    "    #Creamos el modelo\n",
    "    md = Utils.createModel('Model_' + str(dataset_size))\n",
    "    models.append(md)\n",
    "    \n",
    "    #print(len(x_train))\n",
    "    \n",
    "    cb = TimingCallback()\n",
    "    cb.logs = []\n",
    "    #Entrenamos el modelo\n",
    "    history = md.fit(train_gen, epochs=nEpochs, validation_data=val_gen,callbacks=[cb])\n",
    "    #print(cb.logs)\n",
    "    print('\\n\\nEl tiempo total de entrenamiento para esta red sobre un set de datos de ', dataset_size,' individuos ha sido de ',sum(cb.logs),' segundos')\n",
    "    \n",
    "    treshold = 0.90\n",
    "    \n",
    "    #Testamos el modelo\n",
    "    r, FRR, FAR = testModel(md,treshold,x_val,label_val,x_r,y_r,labelPositionDictionary,verb=0)\n",
    "    results.append(r)\n",
    "    #md.evaluate(x_val,label_val)\n",
    "\n",
    "    resultsDF = storeResults(resultsDF, dataset_size, nEpochs, sum(cb.logs), FRR, FAR)\n",
    "    \n",
    "saveResults('Resultados.xlsx',resultsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, FRR, FAR = testModel(md,treshold,x_val,label_val,x_r,y_r,labelPositionDictionary,verb=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probamos la creación de un dataset Aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generando un set de datos de  5  individuos\n",
      "[100   0   0   0]\n",
      "[218   0   0   2]\n",
      "[323   0   1   3]\n",
      "[495   0   0   2]\n",
      "[562   1   1   1]\n",
      "\n",
      "\n",
      "Generando un set de datos de  6  individuos\n",
      "[140   1   0   1]\n",
      "[297   0   1   3]\n",
      "[309   0   0   4]\n",
      "[343   0   0   1]\n",
      "[559   0   1   1]\n",
      "[586   0   1   3]\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = [5,6]\n",
    "for dataset_size in dataset_sizes:\n",
    "    #Generamos un dataset con el tamaño indicado\n",
    "    x_r,y_r,x_e,y_e,x_m, y_m, x_h, y_h = Utils.createDataset_X(\n",
    "        x_real,y_real,x_easy,y_easy,x_medium, y_medium, x_hard, y_hard, dataset_size)\n",
    "    \n",
    "    print('\\n\\nGenerando un set de datos de ',dataset_size,' individuos')\n",
    "\n",
    "    for label in y_r:\n",
    "        print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
